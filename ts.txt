
Troubleshooting most common errors


ContainerConfigError
ImagePullBackOff or ErrImagePull
CrashLoopBackOff
Kubernetes Node Not Ready
Troubleshooting pods
Troubleshooting clusters

CreateContainerConfigError
This error is usually the result of a missing Secret or ConfigMap. Secrets are Kubernetes objects used to store sensitive information like database credentials. ConfigMaps store data as key-value pairs, and are typically used to hold configuration information used by multiple pods.


ImagePullBackOff or ErrImagePull
This status means that a pod could not run because it attempted to pull a container image from a registry, and failed. The pod refuses to start because it cannot create one or more containers defined in its manifest.

How to identify the issue
Run the command kubectl get pods

==================================================================================



CrashLoopBackOff
================================
This issue indicates a pod cannot be scheduled on a node. This could happen because the node does not have sufficient resources to run the pod, or because the pod did not succeed in mounting the requested volumes.

The application inside the container keeps crashing. Here, we can highlight several common situations:

Error in the application configuration. A wrong value or format can make the application exit just after start.

Bugs or not caught exceptions.

One of the downstream services on which the application relies can’t be reached or the connection fails (database, backend, etc.).
Errors in the manifest or pod configuration, such as:
Trying to bind an already used port.
Wrong command arguments for the container.
Errors in liveness probes.
Read-only filesystem.

How to identify the issue
Run the command kubectl get pods.

Check the output to see if the pod status is CrashLoopBackOff

$ kubectl get pods
NAME       READY    STATUS             RESTARTS   AGE
mypod-1    0/1      CrashLoopBackOff   0          58s
Getting detailed information and resolving the issue
Run the kubectl describe pod [name] command for the problematic pod:

The output will help you identify the cause of the issue. Here are the common causes:

Insufficient resources—if there are insufficient resources on the node, you can manually evict pods from the node or scale up your cluster to ensure more nodes are available for your pods.
Volume mounting—if you see the issue is mounting a storage volume, check which volume the pod is trying to mount, ensure it is defined correctly in the pod manifest, and see that a storage volume with those definitions is available.
Use of hostPort—if you are binding pods to a hostPort, you may only be able to schedule one pod per node. In most cases you can avoid using hostPort and use a Service object to enable communication with your pod.
Kubernetes Node Not Ready
When a worker node shuts down or crashes, all stateful pods that reside on it become unavailable, and the node status appears as NotReady.

If a node has a NotReady status for over five minutes (by default), Kubernetes changes the status of pods scheduled on it to Unknown, and attempts to schedule it on another node, with status ContainerCreating.

How to identify the issue
Run the command kubectl get nodes.

Check the output to see is the node status is NotReady

NAME        STATUS      AGE    VERSION
mynode-1    NotReady    1h     v1.2.0
To check if pods scheduled on your node are being moved to other nodes, run the command get pods.







